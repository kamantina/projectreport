{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qlSKlcL-LTMamWQqIUaA4dBmNGcMBTN7",
      "authorship_tag": "ABX9TyO/se9tNg6AHzuaRo1615DW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamantina/projectreport/blob/main/feature_analysis/comment_features_pmi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file with comment data\n",
        "df = pd.read_csv('/content/drive/MyDrive/CISC7298/comments_202050308_utf8_isEnglish2_2050.csv', encoding='utf-8')\n",
        "\n",
        "# Ensure necessary columns exist\n",
        "if 'videoId' not in df.columns or 'textOriginal' not in df.columns:\n",
        "    raise ValueError(\"The required columns 'videoId' and 'textOriginal' are missing in your CSV.\")\n",
        "\n",
        "# Handle missing values\n",
        "df['textOriginal'] = df['textOriginal'].fillna('').astype(str)\n",
        "\n",
        "print(\"Data loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95bhDl76CVXo",
        "outputId": "32da99ba-feca-495b-9402-8cd3bfeae11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18VDJY1VCn-Y",
        "outputId": "9a919000-50dd-4829-a93d-590bc8dfbb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(461534, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess"
      ],
      "metadata": {
        "id": "nc67ePFdZVxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcjIdnC2AVCT",
        "outputId": "ea852d73-473e-44f7-c0cc-43c0ac6a3120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "import contractions\n",
        "\n",
        "\n",
        "# Download stopwords if you haven't already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def safe_expand_contractions(text):\n",
        "    \"\"\"\n",
        "    Safely expand contractions in text using contractions.fix.\n",
        "    If an IndexError (or any Exception) occurs, return the original text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return contractions.fix(text)\n",
        "    except IndexError as e:\n",
        "        print(f\"IndexError in contractions.fix for text: {text} - {e}\")\n",
        "        return text  # Fallback: return text unmodified\n",
        "    except Exception as e:\n",
        "        print(f\"Error in contractions.fix for text: {text} - {e}\")\n",
        "        return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses a text string by lowercasing, removing specific punctuation,\n",
        "    and eliminating stopwords—all while keeping emojis intact.\n",
        "    \"\"\"\n",
        "    # Handle missing or non-string input early\n",
        "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
        "        return []  # Return an empty list for empty or invalid inputs\n",
        "\n",
        "    # Use the safe contraction expansion function\n",
        "    text = safe_expand_contractions(text)\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove ASCII punctuation: this only removes, for instance, !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "    # Emojis (and other non-ASCII characters) are preserved.\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "    # Simple whitespace tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords from the token list\n",
        "    stops = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stops]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the original comment text and store tokens in a new column\n",
        "df['tokens'] = df['textOriginal'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33fc9b3-d0e7-4242-fc41-07aeccfa2b61",
        "id": "i36aDWLqcK1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IndexError in contractions.fix for text: VERY SATİSFYİNG AND COMFORTİNG KİNETİC SAND ASMR # 5 : https://youtu.be/xOvJf_RsUYw - string index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PMI"
      ],
      "metadata": {
        "id": "eQoyD8zCjXEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Candidate Terms"
      ],
      "metadata": {
        "id": "lYINEwRkiPr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps each candidate term (unigram, bigram, and trigram) to a set of comment IDs (ensuring each comment is counted only once per candidate).\n",
        "candidate_dict = defaultdict(set)\n",
        "\n",
        "def add_candidates(comment_id, tokens):\n",
        "    \"\"\"\n",
        "    For a given comment (by comment_id) and its tokenized words,\n",
        "    add unigrams, bigrams, and trigrams (unique within the comment)\n",
        "    to the candidate dictionary.\n",
        "    \"\"\"\n",
        "    # Unigrams\n",
        "    unique_tokens = set(tokens)\n",
        "    for token in unique_tokens:\n",
        "        candidate_dict[token].add(comment_id)\n",
        "\n",
        "    # Bigrams (ordered pairs)\n",
        "    unique_bigrams = set(ngrams(tokens, 2))\n",
        "    for bg in unique_bigrams:\n",
        "        candidate = \" \".join(bg)\n",
        "        candidate_dict[candidate].add(comment_id)\n",
        "\n",
        "    # Trigrams\n",
        "    unique_trigrams = set(ngrams(tokens, 3))\n",
        "    for tg in unique_trigrams:\n",
        "        candidate = \" \".join(tg)\n",
        "        candidate_dict[candidate].add(comment_id)\n",
        "\n",
        "# Process each comment (using the commentId as a unique identifier)\n",
        "for idx, row in df.iterrows():\n",
        "    comment_id = row['commentId']\n",
        "    tokens = row['tokens']\n",
        "    add_candidates(comment_id, tokens)\n",
        "\n",
        "total_comments = len(df)\n",
        "print(f\"Total comments: {total_comments}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318a1541-a524-4ad1-e091-ae6d8cfb2323",
        "id": "fyi21MMjcK1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total comments: 461534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter Candidate Terms by Frequency Criteria"
      ],
      "metadata": {
        "id": "IoVWGr6riTds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidate terms to be kept:\n",
        "#   - Appear in at least 200 comments (to avoid rare words)\n",
        "#   - Appear in no more than 10% of all comments (to filter out overly common words)\n",
        "min_comments = 200\n",
        "max_comments = total_comments / 10\n",
        "\n",
        "filtered_candidates = {\n",
        "    term: len(comment_ids)\n",
        "    for term, comment_ids in candidate_dict.items()\n",
        "    if min_comments <= len(comment_ids) <= max_comments\n",
        "}\n",
        "\n",
        "print(f\"Total candidate terms after frequency filtering: {len(filtered_candidates)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88bb8bb2-60ac-4eb7-ddc9-16a7e824e5f0",
        "id": "kzIJqGj9cK1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total candidate terms after frequency filtering: 2858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute PMI for Each Candidate Relative to Keywords"
      ],
      "metadata": {
        "id": "vGupUmDAifJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keywords for which we want to generate associated word lists.\n",
        "# (They should already be lowercase from preprocessing.)\n",
        "keywords = ['focus', 'relax', 'sleep']\n",
        "\n",
        "# Initialize a dictionary to store PMI scores for each keyword\n",
        "pmi_scores = {kw: {} for kw in keywords}\n",
        "\n",
        "\n",
        "# Here count(k) and count(w) are the number of unique comments in which the keyword or candidate appears, and count(k and w) is the number of comments where both occur.\n",
        "for kw in keywords:\n",
        "    if kw not in candidate_dict:\n",
        "        print(f\"Warning: keyword '{kw}' not found in the data.\")\n",
        "        continue\n",
        "    kw_count = len(candidate_dict[kw])\n",
        "    for term, term_count in filtered_candidates.items():\n",
        "        # Calculate co-occurrence: number of comments containing both the keyword and the candidate term.\n",
        "        # (Both are stored as sets of comment IDs so we take the intersection.)\n",
        "        if term in candidate_dict:\n",
        "            co_occurrence = len(candidate_dict[kw].intersection(candidate_dict[term]))\n",
        "        else:\n",
        "            co_occurrence = 0\n",
        "\n",
        "        if co_occurrence > 0:\n",
        "            pmi = np.log((co_occurrence * total_comments) / (kw_count * term_count))\n",
        "            pmi_scores[kw][term] = pmi\n"
      ],
      "metadata": {
        "id": "Fy8hUa95cK1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the Top n% Candidate Terms by PMI for Each Keyword"
      ],
      "metadata": {
        "id": "cRQq9o8XiriC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_candidate_terms = {}\n",
        "for kw, scores in pmi_scores.items():\n",
        "    if not scores:\n",
        "        continue\n",
        "    # Sort candidate terms for the keyword by PMI in descending order\n",
        "    sorted_terms = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Take the top 4% of these terms\n",
        "    top_n = max(1, int(0.04 * len(sorted_terms)))  # Ensure at least one term is taken\n",
        "    top_candidate_terms[kw] = sorted_terms[:top_n]\n",
        "\n",
        "# # Display the top candidate terms for each keyword along with their PMI scores\n",
        "# for kw, terms in top_candidate_terms.items():\n",
        "#     print(f\"\\nTop candidate terms for keyword '{kw}':\")\n",
        "#     for term, score in terms:\n",
        "#         print(f\"{term}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "kiOmKBd_cK1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Save to CSV with one row per keyword-term-score combination\n",
        "with open('top_candidate_terms.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Keyword', 'Term', 'PMI_Score'])  # Header row\n",
        "\n",
        "    for kw, terms in top_candidate_terms.items():\n",
        "        for term, score in terms:\n",
        "            writer.writerow([kw, term, f\"{score:.4f}\"])"
      ],
      "metadata": {
        "id": "EIPOQ6-Pml9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate feature percentage of each video"
      ],
      "metadata": {
        "id": "vt1eAUpIneEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to count candidate term occurrences in token lists.\n",
        "# This function handles both unigrams and multi-word phrases.\n",
        "def count_candidate_occurrences(tokens, candidate_terms):\n",
        "    count = 0\n",
        "    # Join tokens into a string (with spaces) for simpler phrase matching.\n",
        "    joined_tokens = \" \".join(tokens)\n",
        "\n",
        "    for term in candidate_terms:\n",
        "        term_tokens = term.split()\n",
        "        if len(term_tokens) == 1:\n",
        "            # For single words: count occurrences in the token list.\n",
        "            count += tokens.count(term)\n",
        "        else:\n",
        "            # For phrases: count occurrences in the joined tokens.\n",
        "            # This simple method may count overlapping occurrences.\n",
        "            count += joined_tokens.count(term)\n",
        "    return count\n",
        "\n",
        "# Function to compute the \"feature percentage\" from candidate terms in a list of comments.\n",
        "# Now each comment is already tokenized, so we use the tokens directly.\n",
        "def compute_feature_percentage(video_tokens_list, candidate_terms):\n",
        "    total_token_count = 0\n",
        "    candidate_token_count = 0\n",
        "\n",
        "    # video_tokens_list is a list where each element is a list of tokens from one comment.\n",
        "    for tokens in video_tokens_list:\n",
        "        total_token_count += len(tokens)\n",
        "        candidate_token_count += count_candidate_occurrences(tokens, candidate_terms)\n",
        "\n",
        "    # Return the fraction of candidate term occurrences (multiply by 100 if you want a percent)\n",
        "    return candidate_token_count / total_token_count if total_token_count > 0 else 0\n",
        "\n",
        "# For each keyword, extract just the candidate terms.\n",
        "topic_terms = {}\n",
        "for keyword, term_list in top_candidate_terms.items():\n",
        "    topic_terms[keyword] = [term for term, score in term_list]\n",
        "\n",
        "\n",
        "video_groups = df.groupby('videoId')['tokens'].apply(list)\n",
        "# Now, video_groups is a Series with each value being a list of token lists (one per comment).\n",
        "\n",
        "# Compute the feature percentage for each video for each topic keyword.\n",
        "video_feature_percentages = {}\n",
        "\n",
        "for keyword, candidate_list in topic_terms.items():\n",
        "    video_feature_percentages[keyword] = {}\n",
        "    for videoId, tokens_list in video_groups.items():\n",
        "        percentage = compute_feature_percentage(tokens_list, candidate_list)\n",
        "        video_feature_percentages[keyword][videoId] = percentage\n",
        "\n"
      ],
      "metadata": {
        "id": "hsDFY8C2cK1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary into a DataFrame.\n",
        "# The outer dictionary keys become column names, and the inner dictionary keys become the index.\n",
        "features_df = pd.DataFrame(video_feature_percentages)\n",
        "\n",
        "# Set the index name to 'videoId' and then reset the index so that 'videoId' becomes a column.\n",
        "features_df.index.name = \"videoId\"\n",
        "features_df.reset_index(inplace=True)\n",
        "\n",
        "features_df.head()"
      ],
      "metadata": {
        "id": "20o1vZgocK1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the video data CSV file\n",
        "df_video = pd.read_csv('/content/drive/MyDrive/CISC7298/videos_202500308_utf8_filtered_isEnglish2_2050_LIWC.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "YM-_BPA-cK1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge features_df into videos_df on the column 'videoId'\n",
        "df_video = df_video.merge(features_df, on=\"videoId\", how=\"left\")\n",
        "\n",
        "df_video.head()"
      ],
      "metadata": {
        "id": "byPGtqFHcK1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the video data file with features on focus, relax and sleep\n",
        "df_video.to_csv('videos_202500308_utf8_filtered_isEnglish2_2050_LIWC_pmi200-10-4.csv', index=False)"
      ],
      "metadata": {
        "id": "W0AgO54acK1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}